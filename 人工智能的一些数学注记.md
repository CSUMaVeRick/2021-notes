# 人工智能的一些数学注记

## 拉格朗日乘数法

一般用于带约束条件的优化问题，下面以二元情形为例：

原问题：求解$z=f(x,y)$在条件$\phi(x,y)=0$的极值。

通过构造$F(x,y,\lambda)=f(x,y)+\lambda\phi(x,y)$，然后求解下一方程组：
$$
\left\{\begin{align}\frac{\partial F}{\partial x}=0\\\frac{\partial F}{\partial y}=0\\\frac{\partial F}{\partial\lambda}=0\end{align}\right.\Rightarrow\left\{\begin{array}{}x=\\y=\\\lambda=\end{array}\right.
$$
再将$x,y,\lambda$代入原函数，即得极值。

推广到更多元情形时，只需设更多的独立参数即可。

## 奇异值分解

在数据分析的过程中，当原数据排成的矩阵规模较大时，通过奇异值分解，可将其分为左奇异矩阵、奇异值矩阵、右奇异矩阵，三个较小的矩阵，便于储存和传输。

同时奇异值分解也便于进行对矩阵的数值运算的近似操作。

当矩阵$A$是$n$阶方阵，且可以对角化，那么可将其写作$A=Q\Sigma Q^{-1}$的形式，这称为特征值分解。

这里，$Q$为特征向量组成的矩阵，$\Sigma$为对角元是特征值的对角阵。

奇异值分解就是将特征值分解的方法推广到一般的矩阵上：
$$
A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V^T_{n\times n}
$$
简记为$A=U\Sigma V^T$，$U$和$V$是酉矩阵。

可以通过`numpy.linalg.svd()`进行求解，这个函数的返回值是左奇异矩阵、奇异值、右奇异矩阵。

接下来要用特征值作成奇异值矩阵。

```python
u,s,vt=numpy.linalg.svd(A)
sig=numpy.zeros(numpy.shape(A))
sig[:len(s),:len(s)]=numpy.diag(s)
```

从而我们就能通过三个小矩阵重构原矩阵。

然后我们就可以开始矩阵近似的工作。

```python
#*给定百分比，直接进行近似
def ApproxSVD(data,percent):
    U,s,VT=numpy.linalg.svd(data)
    sigma=numpy.zeros(numpy.shape(data))
    sigma[:len(s),:len(s)]=numpy.diag(s)
    #*这里求出选取的奇异值个数
    sum_sigma=sum(s)
    sum_sigma1=(int)(sum_sigma)*percent
    sum_sigma2=0
    k=0
    while sum_sigma2 < sum_sigma1:
        for i in s:
            sum_sigma2 += i
            k+=1
    #*开始矩阵近似
    D=(U[:,:k].dot(sigma[:k,:k])).dot(VT[:k,:])
    return numpy.rint(D).astype("uint8")
```

我们选择的确定取奇异值个数的方法是取奇异值总和对应百分比分位上的奇异值的个数。

## 相似度算法

这里列出三个：

| 相似度         | 公式                                                         | 说明                                    | 归一化              |
| -------------- | ------------------------------------------------------------ | --------------------------------------- | ------------------- |
| 欧氏距离       | $dist(X,Y)=||X-Y||$                                          | 两个点之间的距离越近，则越相似          | $1/[1+dist(X,Y)]$   |
| 皮尔逊相关系数 | $r=\frac{(X-\bar X)\cdot(Y-\bar Y)}{||X-\bar X||\cdot||Y-\bar Y||}$ | 绝对值越接近于1，相关度越强，反之则越弱 | $0.5+0.5r$          |
| 余弦相似度     | $\cos\theta=\frac{X\cdot Y}{||X||\cdot||Y||}$                | 绝对值越接近于1，相似性越强             | $0.5+0.5\cos\theta$ |

## 极大似然估计&最大后验估计

**似然函数**

离散型：$L(\theta)=\Pi^n_{i=1}p(x_i;\theta)$；连续型：$L(\theta)=\Pi^n_{i=1}f(x_i;\theta)$

**极大似然估计（Maximum Likelihood Estimation，MLE）**

对同一个似然函数，如果存在一个参数值$\hat\theta$，使得似然函数值达到最大的话，那么就称$\hat\theta$为$\theta$的极大似然估计。

求解未知参数的极大似然估计，可以归结为求似然函数$L(x;\theta)$的最值，即$\frac{\mathrm{d}L}{\mathrm d\theta}=0$。由于$\ln L$与$L$有相同的极值点，故也可以取$\frac{\mathrm d\ln L}{\mathrm d\theta}=0$

进而得出$\theta$的估计值。

**最大后验估计（Maximum A Posteriori，MAP）**

利用贝叶斯公式，寻找使后验概率最大的参数。

贝叶斯公式：$P(\theta|X)=\frac{P(\theta)P(X|\theta)}{P(X)}$。

这之中，$P(\theta)$是先验概率，$P(X|\theta)$是似然函数。

上式由于$P(X)$是常数，从而可以通过$P(\theta)P(X|\theta)$来求解。也等价于求解$\ln p(\theta)+\Sigma^n_{i=1}\ln p(x_i|\theta)$的最大值。

最大后验估计加入了先验概率，这在样本量较少的情况下是很有用的。

## 朴素贝叶斯分类器

假如某样本$X$有$n$项特征，分别为$F_1,\dots,F_n$，有$m$个类别，分别为$C_1,\dots,C_m$。**贝叶斯分类器**就是计算出样本$X$后验概率最大的分类，也就是求：
$$
\max\{P(C_i|F_1F_2\cdots F_n),i=1,2,\cdots,m\}
$$
等价于求：
$$
\max\{P(C_i)P(F_1F_2\cdots F_n|C_i),i=1,2,\cdots,m\}
$$
这也就是对于每个类的似然函数与其先验概率之积，寻找那个可以使该式值最大的类。

而朴素贝叶斯假设所有特征都是彼此独立的，从而有$P(F_1F_2\cdots F_n|C_i)=P(F_1|C_i)P(F_2|C_i)\cdots P(F_n|C_i),i=1,2,\cdots,m$

朴素贝叶斯分类器的工作流程如下：

![朴素贝叶斯流程](C:\Downloads\beiyong\朴素贝叶斯流程.jpg)

## 核函数

核函数（Kernel Function）是支持向量机理论的一个重要概念。为了引入对核函数的介绍，需要简单地提及以一些概念。

**超平面**

> 自由度比所在的空间维度小1的几何对象。
>
> 自由度可以理解为是需要给定多少分量才能唯一地确定一个点。比如二维空间的超平面是直线，三维空间的超平面是平面。
>
> 利用解析几何的知识，我们可以这样表示超平面：$\boldsymbol w\cdot\boldsymbol x+b=0$，其中$\boldsymbol w$、$\boldsymbol x$都是$n$维列向量，$\boldsymbol w$可以理解为是“法向量”，也可以认为只是简单的参数。
>
> 另外，由于向量内积可以表示为矩阵乘积，我们也可以这样表达：$\boldsymbol w^T\boldsymbol x +b=0$。

**线性分类&非线性分类**

> 在机器学习中，线性分类器是通过对象特征的线性组合来给出分类的判断。我们之前介绍的朴素贝叶斯分类器就是一种线性分类器。
>
> SVM理论中，如果一个超平面可以将样本完全地区分为两类，就称这些样本是线性可分的。
>
> 然而，也存在许多样本是无法直接线性分类的。比如一个呈圆形散点分布的样本。
>
> 这时就需要非线性分类。
>
> 通过将原样本数据的维度提高，就能做出更有效的分类。这也符合了一种直觉：特征越多，越好分类。
>
> 注意，虽然分类器在这个高维空间里是超平面，但是在原始的空间却并非如此。
>
> 但是如何实现这个思路呢？
>
> 这就来到了这一部分的主题——核函数。

首先给出核函数的定义：

设$\chi$是原始的输入空间，$H$为一个希尔伯特空间，如果有$f(\boldsymbol x):\chi\to H$，使所有的$\boldsymbol x$、$\boldsymbol y\in\chi$，有函数$K(\boldsymbol x,\boldsymbol y)=f(\boldsymbol x)\cdot f(\boldsymbol y)$，那么就称这函数$K(\boldsymbol x,\boldsymbol y)$为核函数

核函数的定义表明，在计算升维后向量的内积时可以使用核函数来代替。

这看起来有些不可思议，但是根据Mercer定理，这种操作是一定可行的！

**Mercer定理**

> 如果$K$是正定对称核，那么必有
> $$
> K(\boldsymbol x,\boldsymbol y)=\sum_{i\in\N}\lambda_ie_i(\boldsymbol x)e_i(\boldsymbol y)
> $$

证明的思路是做特征值分解。

也就是说，我们无需知道具体的升维映射，只需要在原空间$\chi$中使用核函数，就能知道特征空间$H$的内积。

但是要注意，核函数的具体形式和参数是影响到升维映射的，这就导致了核函数性能的差异。

接下来就给出两个常用的核函数：

**多项式核函数**

适合于处理图像。

常用的是一种非齐次形式：
$$
K(\boldsymbol x,\boldsymbol y)=(\boldsymbol x\cdot\boldsymbol y+1)^d
$$
**高斯径向基函数**

适用于数据维数较低的情况，当数据维数较高时，表现较为一般。
$$
K(\boldsymbol x,\boldsymbol y)=e^{-\frac{||\boldsymbol x-\boldsymbol y||^2}{2\sigma^2}}
$$
幂中分子可以看作是向量间的欧氏距离。另外，由于$\sigma$是自由参数，也可以将$\frac1{2\sigma^2}$用一个$\gamma$代替。

```python
import numpy

def poly(X,Y,gamma,c,degree):
    K=X.T.dot(Y)
    K=(gamma*K+c)**degree
    return K
def gaussian(X,Y,sigma):
    K=numpy.exp(-numpy.linalg.norm(X-Y)**2/(2*sigma**2))
    return K

```

## 支持向量机